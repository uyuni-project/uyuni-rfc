- Feature Name: Initial integration with Yomi (MVP)
- Start Date: 2019-10-08
- RFC PR: (leave this empty)

# Summary
[summary]: #summary

Yomi (Yet one more installer) is an installer for openSUSE (Leap,
Tumbleweed, MicroOS) and SLE, expressed as a chain of Salt
states. This RFC propose the integration of Yomi for the provisioning
of new systems in Uyuni.

# Motivation
[motivation]: #motivation

Currently in Uyuni / SUMA we have several ways for installing a new
system:

- For RedHat based systems we can use the Web UI to create, update and
  manage Kickstart files. Those files can be designed to be applied
  for different nodes.

- For [open]SUSE based systems we can upload an AutoYaST file designed
  for a single type of system.

- We can leverage Cobbler to manage the PXE-Boot server and automate
  the installation of new systems (via Kickstart or AutoYaST with
  Cheetah or Jinja2 template engines)

- SUSE Manager for Retail allows the installation of new systems
  dumping a partition image, generated by KIWI from a given template,
  into the local root partition. For this case the PXE boot stack is
  different that the one provided by Cobbler

For one side we can see some unbalance between the Kickstart and the
AutoYaST integration: currently the Kickstart template edition is more
naturally integrated into the UI.

For other side we are moving Uyuni forward into a better and more
consistent integration with SaltStack, but only the retail version
(based on images) is using this configuration management system to
drive the installation.

To improve this situation we propose the integration of the Yomi
installer into Uyuni.

Yomi is a re-formulation of the installation process as a chain of
Salt states. Each state will take responsibility of moving the
installation one step ahead, checking in every step the pre-conditions
and validating the post-conditions.

Yomi is also expressed as a usual Salt formula, so it contains the SLS
state files and the pillar data that parameterize those states. In
this model the pillar will contain the definition of:

- Partitions and file systems used in the node, including the layout
  for LVM and RAID

- Bootloader parameters

- Repositories and packages installed

- Product registration via SUSEConnect

- Services enabled and disabled

- Users and certificates

As we can see, this proposal is (1) moving closer the features of
Kickstart based systems into the SUSE family, and (2) using SaltStack
to orchestrate the new installation.

# Detailed design
[design]: #detailed-design

## Integrating Yomi in Uyuni as a Salt state

The core of the proposal is to add Yomi into the catalog of formulas
available in Uyuni.

As Yomi is indeed a Salt formula, this integration can be done
directly, installing the `yomi-formula` package into the Manager.

The user can later decide to enable the Yomi formula for registered
systems, fill the pillar data using the Formulas with Forms provided
in the package, and launch the installer by applying the highstate.

## Formulas with Forms and sub-forms

Uyuni provides a mechanism, Formula with Forms, that allows the
creation of the pillar data that suit the formula. The current
relation between a form and a Salt state is 1:1, but using some empty
`init.sls` we have the option of splitting a big form into multiple
smaller forms (sub-forms), that optionally belongs to the same group.

Note that sub-forms is an active area of development in Uyuni, so this
will be in place only for the MVP, and later migrated to the accepted
mechanism.

The current `yomi-formula` is using this mechanism to split the pillar
data into six sections:

- yomi: general configuration data, like use `kexec` instead of
  `reboot` after the installation, or some `systemd-firstboot` data.

- yomi-bootloader: indicate the device where the bootloader will be
  installed, and the parameters send to the kernel.

- yomi-services: list of services that will be enabled, and is
  `salt-minion` data will be copied into the new system.

- yomi-software: product and repository registering, and packages
  installation.

- yomi-storage: device partition, LVM or RAID configuration, and file
  systems applied.

- yomi-users: creation of initial users, passwords and certificates.

As we will see in future sections, those forms contains some
overlapping with others Salt formulas (like the locale configuration
that `systemd-firstboot` requires), and with other subsystems in Uyuni
(like the software section).

## Highstate and Workflow

Installing a new system will be equivalent of applying a highstate to
a system booted from the Yomi image.

The current Yomi image is a JeOS based image with a `salt-minion`
service enabled, some systemd overwrites that will help during the
initial configuration, and the CLI tools required when a new
installation is performed. You can find more information here:
https://github.com/openSUSE/yomi#the-iso-image

The current Yomi states are aware of differentiating when the system
is booting from an external image (like the Yomi image), from when the
boot comes from the local device. This mechanism is used to decide
when the installation of a system makes sense or not when a highstate
is re-applied.

This means that we can set all the pillar data for the system, without
risk of breaking or damaging an already installed system.

This open the question about the workflow that will be in place when a
new system is allocated. The current prototype was tested in this way:

1) Boot the new system via the Yomi ISO image. The local DHCP server
   assign a FQDN to the system, and this is used automatically as a
   minion ID.

   (note: the master address was indicated via a GRUB command line
   that a systemd service overwrite in the image can read and
   interpret)

2) Accept the Salt key in the manager (Uyuni 4.0.2), and register the
   new discovered system.

3) Assign to the new system all the Yomi states (Installer group),
   fulfill the forms, and apply the highstate to the new node.

4) Reboot the node, now using the local salt-minion from the new
   system.

Once the new system reboot, the highstate can be re-applied without
consequences, as the state detect early that the root filesystem
device is already mounted, and no subsequent states from Yomi will be
executed.

Some minor bugs in Uyuni (bsc#1150586, bsc#1150589, bsc#1150590,
bsc#1152673, bsc#1152767) makes this process less direct, but for all
except bsc#1152673 Yomi is providing a workaround. The remaining bug
requires some manual intervention before the highstate can be applied
from Uyuni.

As we can see the the next section, this proposal needs to be
validated in relation with the defaul pillar configuration.

As a general rule we want to assign all the configuration data before
the installation of the new system. As we will discuss in the 'Is Yomi
part of the high state?' section, this open the question on where the
Yomi states were will be living and when they will be applied.

## High-level forms?

Another initial problem is that the pillar data is maybe too low-level
for a typical use case. The pillar requires the list of packages that
will be installed, repositories that will be registered, list of
subvolumes for a BtrFS file system, use or not of snapper, partition
layout, etc. You can express MicroOS or classical installations with
those parameters, or create new mixtures of them tailored for specific
use cases that are not directly supported by SUSE.

A, maybe, better approach can be to provide some UI where the user can
select the typical installation profile, similar to the skelcd-control
from YaST. Based on that and the hardware information, a proposal of a
pillar can be generated for this system.

Is not clear to me how we can integrate high-level and low-level forms
inside Uyuni, without making very confusing the usage of the
interface. For example, if we decide to register the repositories from
SLE via a high-level form, maybe is a good idea to provide a mechanism
that will disable the low-level version of the form where the
repositories and packages can be registered, to avoid possible
conflicts.

A better alternative is wait to the integration of saltclass into
Uyuni. This task has been tracked here:

https://github.com/SUSE/spacewalk/issues/9001
https://github.com/SUSE/spacewalk/issues/8367

With saltclass in place, we can potentially define snippets that can
be reused in different places of the pillar data. Via inheritance we
can define generic classes, one for each kind of installation
(MicroOS, SLE, etc), that will be instantiated and altered later, when
the pillar data is generated.

Together with the System Group mechanism, we can define reusable
templates that can be applied (with local changes) into the new
systems.

## Better integration

Another aspect to consider is that this proposal is moving the
installation into the Salt Formula / State realm, and moving it out
from the Systems / Autoinstallation section. Maybe future work can be
done to integrate both sections, providing access to the pillar data
that change the Yomi behavior inside this section.

From the user perspective, the ideal situation is to have a button
'Add new system' as a way to add new deployed systems, similar to the
one that is present for SSH. This should also take a registration code
as a parameter.

## TL;DR

This RFC propose the integration of Yomi in Uyuni in different steps:

1) Do a plain and basic integration of `yomi-formula` in Uyuni, using
   the current limitations of the forms, and the current tested
   deployment workflow. This will need at least the fix of
   bsc#1152673.
   
   The Yomi image needs to be delivered together with Uyuni, together
   with the PXE boot image.

2) When Formula with Forms accept sub-forms, better validators and
   SaltClass support, migrate to it in a different iteration. This
   will include the definition of templates that represent the
   different possible type of [open]SUSE installations.

3) Define a better workflow for the installation, adding a button in
   the system that implement the correct workflow.

# Drawbacks
[drawbacks]: #drawbacks

One of the drawback that I can identify of this proposal is the
integration of yet one more installation mechanism : )

From the user perspective can be confusing the existence of the
Autoinstallation option, the interactions with AutoYaST, Cobbler and
the different alternatives that Uyuni provides.

# Alternatives
[alternatives]: #alternatives

One possible alternative is move out Yomi from the salt-formula space,
and integrate it into the Kickstart / AutoYaST panel.

Internally will still be a Salt state driven by Salt, but we will
provide a Java API similar to those other mechanisms already in place,
that will be used to define the pillars that will be applied when a
new autoinstallation / reinstallation is commanded.

This will imply that the Yomi state is outside the highstate, and that
this new API needs to be designed.

It was noted that a better approach (instead of creating the new API),
is to simplify the UI mid-term by getting rid of all of the separated
out auto-installation stuff.

# Unresolved questions
[unresolved]: #unresolved-questions

Some of the open questions that I am able to identify, are:

## Workflow validation

One potential problem for the proposed workflow has been pointed by
Joachim Werner, and is about the default values for the pillar
data. IIUC the problem is that if Yomi provides some default pillar
configuration, the new discovered node can wrongly apply a highstate
(that AFAIK are applied automatically or scheduled by the user) with
non-optimal data (disk configuration, packages, repositories, etc)

There are two proposals, also from Joe, to address this issue (copied
verbatim from the original post):

-->-->--

1) Discover first.

- Boot a new node (via PXE, UEFI network boot, ISO)

- Register it in SUSE Manager and populate the hardware data

- Shut it down (or leave it running in "wait mode")

- Apply custom configuration by adding formulas and filling in forms
  (could also be "meta-templated" by SUSE Manager's "activation keys"
  that define, channels, admin users, groups)

- Boot if necessary

- Apply highstate, including yomi state

2) Pre-populate the DB:

- Create a new "empty" node in the DB and provide the data needed to
  match it with a physical system (e.g. MAC address or serial number)

- Apply custom configuration by adding formulas and filling in forms
  (could also be "meta-templated" by SUSE Manager's "activation keys"
  that define, channels, admin users, groups)

- Boot node (via PXE, UEFI network boot, ISO)

- Register it in SUSE Manager and populate the hardware data

- Apply highstate, including yomi state

(1) exists as a feature, but currently the "discovery image" isn't
using Salt yet. We could re-use the exact same image that you're using
for installation, just stop before applying the yomi state.

(2) also exists for saltboot. But as of today it's just an API call,
no UI version yet. Trivial to add. What's missing is applying an
activation key to such a system as a template. As of today you'd have
to inject the activation key grain on the minion side before
registering the system to Manager.

--<--<--

The current proposal is similar to the first one, so I expect that the
full idea can work as it is.

## Is Yomi part of the high state?

Currently Yomi can differentiate when is applied in a system that is
booting from the root device, from where is applied when the image is
living in RAM (and consequently the root device can be partitioned,
formatted and chrooted safely)

As expressed before, this mechanics makes safe for the Yomi pillar and
states to be living in the highstate, as then this is applied in an
already installed system, the Yomi states will be disabled (and so we
avoid the risk of breaking a partition and loosing data). Other safe
mechanism are in place too, as today if Yomi detect a mismatch between
the partition schema in the pillar data and in the device, it will
refuse to resize the disk (this will change in the future)

On the other side, we may want to have a full description of the
pillar data for all the formulas (the ones that SUMA use internally,
and the ones from co-installed formulas together with Yomi) that will
be applied into the to-be-deployed system.

Apply a full highstate in a still-to-be-installed node will include
states that are from Yomi and states that are not from Yomi. Those
outside of Yomi are not, by default, aware of the presence of a chroot
environment were the actions needs to be done.

For example if a formula installs apache, it will be installed in the
RAM image; if change a file in /etc/apache2/, it will be changed also
outside the chroot (that usually lives in /mnt), etc.

We have the modules in place that will make possible to fix both
issues, but this require changes in the co-installed formulas, or
changes on how they are invoked.

Another solution is to move out Yomi from the highstate, as proposed
here:

https://github.com/SUSE/spacewalk/issues/2714

In this model, we still define all the provisioning data before the
allocation of the node. The Yomi states can be applied only when we
command a installation / re-installation of the system, and the states
that are living in the highstate will be applied on the already
installed system (after the reboot).

## How the minion can set the minion ID automatically.

The current Yomi image (PXE boot and ISO), provides a systemd
overwrite that is executed before the salt-minion service. This piece
of code will set a minion ID for the system based on some rules:

- Use the ID injected via the bootloader parameter 'minion='

- If a FQDN is provided via DHCP, this name will be used as the ID

- If no parameter or of the hostname is localhost, the MAC address
  will be used as minion ID

The question is to validate those rules for the general use case.

## How the minion can find the master automatically.

In a similar way, an systemd overlay will take care of configuring
where to find the salt-master node.

- If the bootloader parameter 'master=' is provided, a new file
  'master.conf' will be added inside '/etc/salt/minion.d'

- If not, the default 'salt' name will be used

To increase the security, we can inject the master's public key in the
RAM image and enable strict checking.

## Disable SSH by default.

The current PXE Boot and ISO image contains a root user than can be
accessed without password. The problem is that as the SSH service is
enabled, you can remote login into the new system as root without
password.

One solution is to disable password-less login via SSH, and inject
some parameters that can enable the login, or authorized certificates.

We can think of different mechanisms that can inject this metadata
early during the boot process, in a similar way that is done in
`cloud-init` or `ignition`. The payload, delivered via an URL, TXT
entry from DNS, or extra parameters inside DHCP (RFC 2132) can provide
the address of the master node. This problem will be move out of the
scope of the proposal.

## Simplified forms

The prototype provide a set of subforms that cover all the pillar data
that Yomi can understand. Maybe there is the option of hidding those
forms and provide a simplified version, based of a single option like:
(1) what kind of OS want to be installed, (2) simplified partition
definition.

The list of packages, repositories, service configuration and other
elements will be described and configured by current parts of Uyuni,
after the first boot.

## Workflow

In the 'Detailed design' section we pointed a problem with the naive
workflow, and two better alternatives were provided.

We can provide one or both workflows enabling new UI options. It is
recommended to have a button 'Add new system' as a way to add new
deployed systems, similar to the one that is present for SSH. This
should also take a registration code as a parameter.

## Overlap with different pillar and formula

The pillar data for Yomi requires the definition of the language,
keyboard layout to use, timezone, list of packages, etc. This can
overlap with other formulas like 'locate', or some other components of
Uyuni like 'Sofware'

How to resolve this overlap is still an open question.

## Be ready for Redfish integration

In the future maybe we will want to integrate more different ways of
installation, like Redfish, where Yomi can have a role on
it. Decisions done in this stage can influence the possibilities that
we will have in the future.

In the first iteration, are not expect too many complications with
Redfish. Redfish will mainly allow for three things that could be
useful for us in the Yomi scenario:

1) HTTP(S) boot (technically not a Redfish feature, but all servers
   that support Redfish also have an HTTP(S) boot enabled UEFI
   firmware, and we can use Redfish to put the server into HTTP boot
   mode). In a nutshell, HTTP boot is really just PXE boot with HTTP
   instead of TFTP. This will allow to directly boot the full image
   from http(s), without having to chain-load it via TFTP =>
   iPXE. Larger images are allowed, and the transmission can be
   secured with TLS (provided that the admin correctly deploys the
   certificates via Redfish as well).

2) Set certain parameters on the system that can later be read from
   the RAM disk image (e.g. we could use that to deploy an activation
   key grain to the firmware, or set a hostname that the image can
   pick up).

3) At least in some vendor extensions, you can directly point Redfish
   to a URL with the boot media, no need for PXE or http boot.

Finally, you can also switch the system on via Redfish and do firmware
updates or configure RAID controllers.
