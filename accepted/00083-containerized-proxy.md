- Feature Name: Deliver Proxy as a container
- Start Date: 2021-10-11

# Summary
[summary]: #summary

Deliver Proxy as a containerized application.

# Motivation
[motivation]: #motivation

There are deployment scenarios in which adding a new Virtual Machine to implement Proxy functionality is cumbersome or impossible, while leveraging an existing container runtime cold be an easier option, especially in highly distributed ("Edge") environments.

Having the option of deploying Proxy functionality as containers can be easier to operate (eg. deployment, independence from underlying OS, keeping up to date) and more flexible (eg. implementing active/passive HA by leveraging an existing Kubernetes environment).

It is believed implementing Proxy containerization will surface a subset of common problems and solutions also useful to implement a containerized Server in future.

# Detailed design
[design]: #detailed-design

## Delivery
  - Proxy is delivered as four containers:
    1. Apache `httpd` and Proxy-specific Python WSGI applications
    2. Squid
    3. `salt-broker`
    4. `tftpd`
  - Updates are delivered via new images on a public Registry
  - [podman](https://podman.io/)-based systemd units are provided for orchestation on distros which support podman (note that [podman supports generation of units which "wrap" containers](https://www.redhat.com/sysadmin/podman-shareable-systemd-services))
  - A minimal [Helm](https://helm.sh/) chart, tested against [k3s](https://k3s.io/), is provided for Kubernetes environments
  - Minimal documentation to execute on [docker](https://docs.docker.com/engine/reference/commandline/cli/) is also provided

## Onboarding
  - Containerized Proxy is not a managed client, does not run `salt-minion`, is not registered in the usual way
  - A new UI page (and API endpoint) allows to add containerized Proxies by specifying their FQDN
    - After confirmation a System entry (in the System list) is created 
      - Normal client actions (package management, configuration file management...) will not be available on that System
      - Proxy-specific Server UI/API functionality, like showing the list of attached clients, will be available like on regular Proxies
    - for each such System, the Server generates a tarball ("configuration directory") that can be downloaded
    - Proxy containers need the configuration directory to be mounted as a volume. After start, it is fully connected and operational

## Operating
  1. Apache `httpd` and Proxy-specific Python WSGI applications container
    - The following ports will be exposed:
      - TCP 80: http, for clients to download packages (during automated installations)
      - TCP 443: https, for clients to download packages, repository metadata, etc.
    - The following volumes will be mounted:
      - one with the configuration directory
      - one with cached content (`/var/cache/rhn`, `/srv/tftpboot`)
      - logs will be sent to standard output/error ([like the official image does](https://github.com/docker-library/httpd/blob/cab17d54f9e0070c672326a555996d94922b486e/2.4/Dockerfile#L203-L205))
  2. Squid
    - The following ports will be exposed:
      - TCP 8080: http, used by the Apache `httpd` container only
    - The following volumes will be mounted:
      - one with the configuration directory
      - one with cached content (`/var/cache/squid/`)
      - logs will be sent to standard output/error (see above)
  3. `salt-broker`
    - The following ports will be exposed:
      - TCP 4505 and TCP 4506: Salt zeromq
    - The following volumes will be mounted:
      - one with the configuration directory
      - logs will be sent to standard output (requires patching `salt-broker`)
  4. `tftpd`
    - The following ports will be exposed:
      - UDP 69: tftp
    - The following volumes will be mounted:
      - one with the configuration directory
      - one with cached content to serve (`/srv/tftpboot`, read-only)
      - logs will be sent to standard output

  - Proxy Containers can be started/stopped/exchanged - as long as the configuration directory stays the same the Proxy identity, history and functionality will be preserved. Optionally, the cached content directory can also be preserved for optimal performance.
    - note that `cobbler sync` will need a new execution from the Server for refreshing `/srv/tftpboot`
  - Creating bootstrap scripts from containerized Proxies will not be possible. `mgr-bootstrap --hostname=$PROXY_NAME` from the Server will still be possible
  - SSL certificates can be handled in three ways:
    1. using the same CA as the Server, as generated at Server installation time. A new SSL certificate for the Proxy is generated at "configuration tarball" creation time. This will be the default option
    2. using the same CA as the Server, as generated by the user ("bring your own SSL certs"). A new SSL certificate for the Proxy has to be placed in the configuration directory by the user
    3. leave SSL termination to the user's infrastructure altogether

## Limitations
  - Only ZeroMQ Salt clients are supported
  - No monitoring functionality
  - No SLP functionality
  - Traceback emails are not sent
  - `rhn_package_manager` is not present nor supported

## Later steps
  - add Salt SSH support
  - Retail Branch Server: to be defined in a separate RFC
  - add support for Apache and Squid Prometheus exporters for monitoring
  - enable the use of "configuration directories" for VM Proxies as well

## Other details
  - base image for all containers will be Leap 15.3
  - API/UI will need the following data to add a Proxy:
    - FQDN
    - optional details of a plain http proxy to reach the Server (FQDN, username and password)
    - choice for the SSL cert: generated or bring-your-own-certificate
      1. if generated: [CNAMEs](https://en.wikipedia.org/wiki/CNAME_record), Common Name, Organization, OU, City, State, Country Code, e-mail, Password
      2. if bring-your-own-certificate: nothing (with an explaination on where the files need to be placed should be displayed)
      3. no SSL termination: nothing (with an explanation on having to provide termination via other means)
    - checkbox to optionally configure the proxy for [tftpsync](https://github.com/uyuni-project/uyuni/blob/master/tftpsync/susemanager-tftpsync/configure-tftpsync.sh) ([docs](https://www.uyuni-project.org/uyuni-docs/en/uyuni/installation/uyuni-proxy-setup.html#proxy.pxe.sync))
  - Building of the image will happen in the Build Service, by and large leveraging existing packages

# Drawbacks
[drawbacks]: #drawbacks

  - Long term this is a separate deployment mechanism to maintain and support. In future it should be decided whether keeping both is worth it

# Alternatives
[alternatives]: #alternatives

  - Proxy could be delivered as one "system container".
    - pro: deployment is easier
    - con: the one image will be bigger, resulting in more network load on updates
    - con: it might need an `init` process
    - con: in k8s environments, it is recommended for containers to have all their logs to standard output (in order to use native tools for log processing and aggregation). Having multiple process per container makes it more likely this is not possible and file-based solutions are needed instead
    - con: in k8s environments, it is recommended that containers start quickly. Having multiple process per container makes it more likely for the service in it to take more time to be ready
    - con: in k8s environments, it is customary to set per-container memory limits. Having multiple process per container makes setting a value more difficult


# Unresolved questions
[unresolved]: #unresolved-questions

None known at the time of writing.
