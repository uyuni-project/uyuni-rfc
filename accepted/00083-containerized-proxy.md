- Feature Name: Deliver Proxy as a container
- Start Date: 2021-10-11

# Summary
[summary]: #summary

Deliver Proxy as a containerized application.

# Motivation
[motivation]: #motivation

There are deployment scenarios in which adding a new Virtual Machine to implement Proxy functionality is cumbersome or impossible, while leveraging an existing container runtime cold be an easier option, especially in highly distributed ("Edge") environments.

Having the option of deploying Proxy functionality as containers can be easier to operate (eg. deployment, independence from underlying OS, keeping up to date) and more flexible (eg. implementing active/passive HA by leveraging an existing Kubernetes environment).

It is believed implementing Proxy containerization will surface a subset of common problems and solutions also useful to implement a containerized Server in future.

# Detailed design
[design]: #detailed-design

## Delivery
  - Proxy is delivered as one container
    - Updates are delivered via new images on a public Registry
  - A [podman](https://podman.io/)-based systemd unit for easy execution on distros which support podman (note that [podman supports generation of units which "wrap" containers](https://www.redhat.com/sysadmin/podman-shareable-systemd-services))
  - A minimal [Helm](https://helm.sh/) chart, tested against [k3s](https://k3s.io/), for Kubernetes environments
  - Minimal documentation to execute on [docker](https://docs.docker.com/engine/reference/commandline/cli/)

## Onboarding
  - Containerized Proxy is not a managed client, does not run `salt-minion`, is not registered in the usual way
  - A new UI page (and API endpoint) allows to add containerized Proxies by specifying their FQDN
    - After confirmation a System entry (in the System list) is created 
      - Normal client actions (package management, configuration file management...) will not be available on that System
      - Proxy-specific Server UI/API functionality, like showing the list of attached clients, will be available like on regular Proxies
    - for each such System, the Server generates a tarball ("configuration directory") that can be downloaded
    - Proxy container needs the configuration directory to be mounted as a volume. After start, it is fully connected and operational

## Operating
  - The following ports will be exposed:
    - TCP 4505 and TCP 4506: Salt zeromq
    - TCP 80: http, for clients to download packages (during automated installations)
    - TCP 443: https, for clients to download packages, repository metadata, etc.
    - UDP 69: tftp, for clients to perform PXE (network) boot
  - The following volumes will be mounted:
    - one with the configuration directory
    - one with cached content (`/var/cache/squid/`, `/var/cache/rhn`, `/srv/tftpboot`)
    - one with log files (`/var/log/apache2`, `/var/log/squid`, `salt-broker` log)
  - Proxy Containers can be started/stopped/exchanged - as long as the configuration directory stays the same the Proxy identity, history and functionality will be preserved. Optionally, the cached content directory can also be preserved for optimal performance.
    - note that `cobbler sync` will need a new execution from the Server for refreshing `/srv/tftpboot`
  - Creating bootstrap scripts from containerized Proxies will not be possible. `mgr-bootstrap --hostname=$PROXY_NAME` from the Server will still be possible
  - SSL certificates can be handled in three ways:
    1. using the same CA as the Server, as generated at Server installation time. A new SSL certificate for the Proxy is generated at "configuration tarball" creation time. This will be the default option
    2. using the same CA as the Server, as generated by the user ("bring your own SSL certs"). A new SSL certificate for the Proxy has to be placed in the configuration directory by the user
    3. leave SSL termination to the user's infrastructure altogether

## Limitations
  - Only ZeroMQ Salt clients are supported
  - No monitoring functionality
  - No SLP functionality
  - Traceback emails are not sent
  - `rhn_package_manager` is not present nor supported

## Later steps
  - add Salt SSH support
  - Retail Branch Server: to be defined in a separate RFC
  - add support for Apache and Squid Prometheus exporters for monitoring
  - enable the use of "configuration directories" for VM Proxies as well

## Other details
  - Container image includes:
    - Apache `httpd`, together with Proxy-specific Python WSGI applications
    - `salt-broker`
    - `tftpd`
    - an `init` process such as [s6](https://github.com/just-containers/s6-overlay) to [forward termination signals correctly](https://petermalmgren.com/signal-handling-docker/) and to [reap zombie processes](https://stackoverflow.com/questions/49162358/docker-init-zombies-why-does-it-matter)
  - base image will be Leap 15.3
  - API/UI will need the following data to add a Proxy:
    - FQDN
    - optional details of a plain http proxy to reach the Server (FQDN, username and password)
    - choice for the SSL cert: generated or bring-your-own-certificate
      1. if generated: [CNAMEs](https://en.wikipedia.org/wiki/CNAME_record), Common Name, Organization, OU, City, State, Country Code, e-mail, Password
      2. if bring-your-own-certificate: nothing (with an explaination on where the files need to be placed should be displayed)
      3. no SSL termination: nothing (with an explanation on having to provide termination via other means)
    - checkbox to optionally configure the proxy for [tftpsync](https://github.com/uyuni-project/uyuni/blob/master/tftpsync/susemanager-tftpsync/configure-tftpsync.sh) ([docs](https://www.uyuni-project.org/uyuni-docs/en/uyuni/installation/uyuni-proxy-setup.html#proxy.pxe.sync))
  - Building of the image will happen in the Build Service, by and large leveraging existing packages

# Drawbacks
[drawbacks]: #drawbacks

  - Long term this is a separate deployment mechanism to maintain and support. In future it should be decided whether keeping both is worth it

# Alternatives
[alternatives]: #alternatives

  - Proxy could be split into more containers, according to the microservices pattern.
    - con: this would make deployment and operations more complex
    - pro: individual images would be smaller, resulting in less network load on updates
    - pro: it might not need an `init` process
    - pro: in k8s environments, it is recommended for containers to have all their logs to standard output (in order to use native tools for log processing and aggregation). Having one process per container makes it more likely that no log files are actually needed
    - pro: in k8s environments, it is recommended that containers start quickly. Having one process per container makes it more likely for the service in it to be ready more quickly
    - pro: in k8s environments, it is customary to set per-container memory limits. Having one process per container makes setting a value easier


# Unresolved questions
[unresolved]: #unresolved-questions

None known at the time of writing.
